{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3fbcdb",
   "metadata": {},
   "source": [
    "# Basic Blocking Program Example\n",
    "\n",
    "In this example we will show how to create and run a simple blocking program locally. This example proceeds in the following steps,\n",
    "\n",
    "0. Setup\n",
    "1. Initialize Spark\n",
    "2. Read the Data\n",
    "3. Create a Blocking Program\n",
    "4. Execute a Blocking Program\n",
    "5. Compute Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98313b4",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e143869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = str(Path().resolve().parent)\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "from delex.lang.predicate import (\n",
    "        BM25TopkPredicate,\n",
    "        JaccardPredicate,\n",
    "        EditDistancePredicate,\n",
    "        SmithWatermanPredicate,\n",
    "        JaroPredicate, \n",
    "        JaroWinklerPredicate, \n",
    "        CosinePredicate, \n",
    "        ExactMatchPredicate\n",
    ")\n",
    "\n",
    "from delex.lang import BlockingProgram, DropRule, KeepRule\n",
    "from delex.tokenizer import StrippedWhiteSpaceTokenizer, QGramTokenizer\n",
    "from delex.execution.plan_executor import PlanExecutor\n",
    "import operator\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d0b5c6",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark\n",
    "\n",
    "Next we need to initialize Spark. For this example we are doing everything in a local setup, in particular, all files are stored on the local file system and we are running Spark in local mode. In order to run on a cluster the SparkSession should be initialized with the correct master and the files must be stored where all nodes have access to them (e.g. HDFS, PostgresSQL, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable pyarrow execution, recommended for better performance\n",
    "conf = SparkConf()\\\n",
    "        .set('spark.sql.execution.arrow.pyspark.enabled',  'true')\n",
    "\n",
    "# initialize a local spark context\n",
    "spark = SparkSession.builder\\\n",
    "                    .master('local[*]')\\\n",
    "                    .config(conf=conf)\\\n",
    "                    .appName('Basic Example')\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a8217",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "With the repository we have provided a small sample dataset called `dblp_acm` in parquet format. This is a small dataset of paper citations with about 1000 rows per table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the test data directory\n",
    "data_path = (Path().resolve() / 'data' / 'dblp_acm').absolute()\n",
    "\n",
    "# table to be indexed, generally this should be the table with fewer rows\n",
    "index_table_path = data_path / 'table_a.parquet'\n",
    "# table for searching\n",
    "search_table_path = data_path / 'table_b.parquet'\n",
    "# the ground truth, i.e. the correct matching pairs\n",
    "gold_path = data_path / 'gold.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eb970f",
   "metadata": {},
   "source": [
    "## 2. Read the Data\n",
    "\n",
    "Once Spark is initialized, we can then read all of our data into Spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the data as spark dataframes\n",
    "index_table = spark.read.parquet(f'file://{str(index_table_path)}')\n",
    "search_table = spark.read.parquet(f'file://{str(search_table_path)}')\n",
    "gold = spark.read.parquet(f'file://{str(gold_path)}')\n",
    "\n",
    "index_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9a9a54",
   "metadata": {},
   "source": [
    "## 3. Create a Blocking Program\n",
    "\n",
    "Next we need to define our blocking program. For this basic example, we will define a very simple blocking program that returns all pairs where the Jaccard scores using a 3gram tokenizer are greater than or equal to .6. To do this we define a `BlockingProgram` with a single `KeepRule` which has a single `JaccardPredicate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prog = BlockingProgram(\n",
    "        keep_rules = [\n",
    "                KeepRule([ JaccardPredicate('title', 'title', QGramTokenizer(3), operator.ge, .6)])\n",
    "            ],\n",
    "        drop_rules = [],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b8b02d",
   "metadata": {},
   "source": [
    "In terms of SQL this program is equivalent to\n",
    "\n",
    "```SQL\n",
    "SELECT A.id, B.id\n",
    "FROM index_table as A, search_table as B\n",
    "WHERE jaccard_3gram(A.title, B.title) >= .6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc59a892",
   "metadata": {},
   "source": [
    "## 4. Execute a Blocking Program\n",
    "\n",
    "Next, we create a `PlanExecutor` and execute the `BlockingProgram` by calling `.execute()`. Notice, that we passed `optimize=False` and `estimate_cost=False` as arguments, these parameters control the plan that is generated, which will be explained in a separate example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f402780",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = PlanExecutor(\n",
    "        index_table=index_table, \n",
    "        search_table=search_table,\n",
    "        optimize=False,\n",
    "        estimate_cost=False,\n",
    ")\n",
    "\n",
    "candidates, stats = executor.execute(prog, ['_id'])\n",
    "candidates = candidates.persist()\n",
    "candidates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677cc437",
   "metadata": {},
   "source": [
    "## 5. Compute Recall\n",
    "\n",
    "Finally, we can compute recall. As you can see, the output of the `PlanExecutor` is actually grouped by the id of `search_table`. This is done for space and computation effeicency reasons. To compute recall we first need to 'unroll' the output and then do a set intersection with the gold pairs to get the number of true positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unroll the output\n",
    "pairs = candidates.select(\n",
    "                    F.explode('ids').alias('a_id'),\n",
    "                    F.col('_id').alias('b_id')\n",
    "                )\n",
    "# total number \n",
    "gold = gold.drop("__index_level_0__")\n"
    "n_pairs = pairs.count()\n",
    "true_positives = gold.intersect(pairs).count()\n",
    "recall = true_positives / gold.count()\n",
    "print(f'n_pairs : {n_pairs}')\n",
    "print(f'true_positives : {true_positives}')\n",
    "print(f'recall : {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the dataframe from the cache\n",
    "candidates.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
